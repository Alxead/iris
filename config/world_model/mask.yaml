_target_: models.MaskTransformerConfig
tokens_per_block: 16
max_blocks: 4   # history length
encoder_num_layers: 6
decoder_num_layers: 6
num_heads: 4
embed_dim: 256
embed_pdrop: 0.1
resid_pdrop: 0.1
attn_pdrop: 0.1
num_iter: 6